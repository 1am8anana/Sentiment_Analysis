{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import feature variables that i just prepared for the model experiments from preprocess.py file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import preprocess as pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tcas data\n",
    "X1_train = pp.BOWtcas_train\n",
    "X1_test = pp.BOWtcas_test\n",
    "y1_train = pp.ytcas_train\n",
    "y1_test = pp.ytcas_test\n",
    "\n",
    "# combined data\n",
    "X2_train = pp.BOWdf_train\n",
    "X2_test = pp.BOWdf_test\n",
    "y2_train = pp.ydf_train\n",
    "y2_test = pp.ydf_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Modelling**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now i'll instantiate the model that can works with my features, for easy to build i choose sklearn for instantiatation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import xgboost as xgb\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=5, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Start with  tcas data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 0 ns\n",
      "{'lr__C': 1, 'lr__penalty': 'l2', 'lr__solver': 'lbfgs'}\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "lr_pipeline = Pipeline([('lr', LogisticRegression())])\n",
    "param_lr = {\n",
    "    'lr__C': [0.1, 1, 10],\n",
    "    'lr__penalty': ['l2'],\n",
    "    'lr__solver': ['lbfgs'] \n",
    "}\n",
    "# Initialize GridSearchCV\n",
    "lr_grid_search = GridSearchCV(lr_pipeline, param_lr, cv=kf, scoring='accuracy')\n",
    "# Perform GridSearchCV to find the best combination of hyperparameters\n",
    "lr_grid_search.fit(X1_train, y1_train)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "print(lr_grid_search.best_params_)\n",
    "best_lr_pipeline = lr_grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 0 ns\n",
      "{'nb__alpha': 1.0}\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "nb_pipeline = Pipeline([('nb', MultinomialNB())])\n",
    "param_nb = {\n",
    "    'nb__alpha': [0.1, 1.0, 10.0]\n",
    "}\n",
    "# Initialize GridSearchCV\n",
    "nb_grid_search = GridSearchCV(nb_pipeline, param_nb, cv=kf, scoring='accuracy')\n",
    "# Perform GridSearchCV to find the best combination of hyperparameters\n",
    "nb_grid_search.fit(X1_train, y1_train)\n",
    "# Get the best hyperparameters\n",
    "print(nb_grid_search.best_params_)\n",
    "best_nb_pipeline = nb_grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 0 ns\n",
      "{'rf__max_depth': None, 'rf__n_estimators': 300}\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "rf_pipeline = Pipeline([('rf', RandomForestClassifier())])\n",
    "param_rf = {\n",
    "    'rf__n_estimators': [100, 200, 300],\n",
    "    'rf__max_depth': [None, 10, 20],\n",
    "}\n",
    "rf_grid_search = GridSearchCV(rf_pipeline, param_rf, cv=kf, scoring='accuracy')\n",
    "# Perform GridSearchCV to find the best combination of hyperparameters\n",
    "rf_grid_search.fit(X1_train, y1_train)\n",
    "# Get the best hyperparameters\n",
    "print(rf_grid_search.best_params_)\n",
    "best_rf_pipeline = rf_grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 0 ns\n",
      "{'xg__learning_rate': 0.1, 'xg__max_depth': 3, 'xg__n_estimators': 100}\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "xg_pipeline = Pipeline([('xg', xgb.XGBClassifier())])\n",
    "param_xg = {\n",
    "    'xg__n_estimators': [100, 200, 300],\n",
    "    'xg__max_depth': [3, 5, 7],\n",
    "    'xg__learning_rate': [0.1, 0.01, 0.001]\n",
    "}\n",
    "xg_grid_search = GridSearchCV(xg_pipeline, param_xg, cv=kf, scoring='accuracy')\n",
    "# Perform GridSearchCV to find the best combination of hyperparameters\n",
    "xg_grid_search.fit(X1_train, y1_train)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "print(xg_grid_search.best_params_)\n",
    "best_xg_pipeline = xg_grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 0 ns\n",
      "{'nn__activation': 'logistic', 'nn__hidden_layer_sizes': (50, 50, 50), 'nn__max_iter': 200, 'nn__solver': 'adam'}\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "nn_pipeline = Pipeline([('nn', MLPClassifier())])\n",
    "param_nn = {\n",
    "    'nn__hidden_layer_sizes': [(50,), (100,), (50, 50), (100, 100), (50, 50, 50)],\n",
    "    'nn__activation': ['relu', 'tanh', 'logistic'],\n",
    "    'nn__solver': ['adam', 'sgd'],\n",
    "    'nn__max_iter': [200, 500, 1000],\n",
    "}\n",
    "nn_grid_search = GridSearchCV(nn_pipeline, param_nn, cv=kf, scoring='accuracy')\n",
    "nn_grid_search.fit(X1_train, y1_train)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "print(nn_grid_search.best_params_)\n",
    "best_nn_pipeline = nn_grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Lastly does with combined data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 0 ns\n",
      "{'lrc__C': 10, 'lrc__penalty': 'l2', 'lrc__solver': 'lbfgs'}\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "lrc_pipeline = Pipeline([('lrc', LogisticRegression())])\n",
    "param_lrc = {\n",
    "    'lrc__C': [0.1, 1, 10],\n",
    "    'lrc__penalty': ['l2'],\n",
    "    'lrc__solver': ['lbfgs'] \n",
    "}\n",
    "# Initialize GridSearchCV\n",
    "lrc_grid_search = GridSearchCV(lrc_pipeline, param_lrc, cv=kf, scoring='accuracy')\n",
    "# Perform GridSearchCV to find the best combination of hyperparameters\n",
    "lrc_grid_search.fit(X2_train, y2_train)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "print(lrc_grid_search.best_params_)\n",
    "best_lrc_pipeline = lrc_grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 0 ns\n",
      "{'nbc__alpha': 1.0}\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "nbc_pipeline = Pipeline([('nbc', MultinomialNB())])\n",
    "param_nbc = {\n",
    "    'nbc__alpha': [0.1, 1.0, 10.0]\n",
    "}\n",
    "# Initialize GridSearchCV\n",
    "nbc_grid_search = GridSearchCV(nbc_pipeline, param_nbc, cv=kf, scoring='accuracy')\n",
    "# Perform GridSearchCV to find the best combination of hyperparameters\n",
    "nbc_grid_search.fit(X2_train, y2_train)\n",
    "# Get the best hyperparameters\n",
    "print(nbc_grid_search.best_params_)\n",
    "best_nbc_pipeline = nbc_grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 0 ns\n",
      "{'rfc__max_depth': None, 'rfc__n_estimators': 300}\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "rfc_pipeline = Pipeline([('rfc', RandomForestClassifier())])\n",
    "param_rfc = {\n",
    "    'rfc__n_estimators': [100, 200, 300],\n",
    "    'rfc__max_depth': [None, 10, 20],\n",
    "}\n",
    "rfc_grid_search = GridSearchCV(rfc_pipeline, param_rfc, cv=kf, scoring='accuracy')\n",
    "# Perform GridSearchCV to find the best combination of hyperparameters\n",
    "rfc_grid_search.fit(X2_train, y2_train)\n",
    "# Get the best hyperparameters\n",
    "print(rfc_grid_search.best_params_)\n",
    "best_rfc_pipeline = rfc_grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 0 ns\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'xgc__learning_rate': 0.1, 'xgc__max_depth': 3, 'xgc__n_estimators': 300}\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "xgc_pipeline = Pipeline([('xgc', xgb.XGBClassifier())])\n",
    "param_xgc = {\n",
    "    'xgc__n_estimators': [100, 200, 300],\n",
    "    'xgc__max_depth': [3, 5, 7],\n",
    "    'xgc__learning_rate': [0.1, 0.01, 0.001]\n",
    "}\n",
    "xgc_grid_search = GridSearchCV(xgc_pipeline, param_xgc, cv=kf, scoring='accuracy')\n",
    "# Perform GridSearchCV to find the best combination of hyperparameters\n",
    "xgc_grid_search.fit(X2_train, y2_train)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "print(xgc_grid_search.best_params_)\n",
    "best_xgc_pipeline = xgc_grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 0 ns\n",
      "{'nnc__activation': 'relu', 'nnc__hidden_layer_sizes': (50,), 'nnc__max_iter': 500, 'nnc__solver': 'adam'}\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "nnc_pipeline = Pipeline([('nnc', MLPClassifier())])\n",
    "param_nnc = {\n",
    "    'nnc__hidden_layer_sizes': [(50,), (100,), (50, 50), (100, 100), (50, 50, 50)],\n",
    "    'nnc__activation': ['relu', 'tanh', 'logistic'],\n",
    "    'nnc__solver': ['adam', 'sgd'],\n",
    "    'nnc__max_iter': [200, 500, 1000],\n",
    "}\n",
    "                        \n",
    "nnc_grid_search = GridSearchCV(nnc_pipeline, param_nnc, cv=kf, scoring='accuracy')\n",
    "nnc_grid_search.fit(X2_train, y2_train)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "print(nnc_grid_search.best_params_)\n",
    "best_nnc_pipeline = nnc_grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*End of Modelling*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['best_nnc_pipeline.pkl']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Save the trained pipeline to a file\n",
    "\n",
    "# model pipeline wit tcas trained \n",
    "joblib.dump(best_lr_pipeline, 'best_lr_pipeline.pkl')\n",
    "joblib.dump(best_nb_pipeline, 'best_nb_pipeline.pkl')\n",
    "joblib.dump(best_rf_pipeline, 'best_rf_pipeline.pkl')\n",
    "joblib.dump(best_xg_pipeline, 'best_xg_pipeline.pkl')\n",
    "joblib.dump(best_nn_pipeline, 'best_nn_pipeline.pkl')\n",
    "\n",
    "# model pipeline wit combined trained\n",
    "joblib.dump(best_lrc_pipeline, 'best_lrc_pipeline.pkl')\n",
    "joblib.dump(best_nbc_pipeline, 'best_nbc_pipeline.pkl')\n",
    "joblib.dump(best_rfc_pipeline, 'best_rfc_pipeline.pkl')\n",
    "joblib.dump(best_xgc_pipeline, 'best_xgc_pipeline.pkl')\n",
    "joblib.dump(best_nnc_pipeline, 'best_nnc_pipeline.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
